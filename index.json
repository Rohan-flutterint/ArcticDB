{"project": "ArcticDB", "project_url": "http://arcticdb.io/", "show_commit_url": "https://github.com/man-group/ArcticDB/commit/", "hash_length": 8, "revision_to_hash": {"42": "8aca4e9cc678650de36a12d31d2e7f3c4cc9a03b", "214": "b70029d5724883dc903fc6a9f95b17de33dde93d", "292": "03b8e00539fc031dcabd3b2ec3accfbc9e8e3587", "308": "bf01831fd860837eec04f0437ee7ee54aca60d00", "436": "c90a21a611b5c6ec2ef4b049981ac5c2ccb8ad08", "499": "4daa3de40dd67ebde39d48797ce05240dba1e05a", "504": "14209d33c87c3325c5a77879a3cfa036894fea52", "506": "95806216ea1bdfbbc0930453adc1db42d3a4e624", "508": "525e6c28ae67faa1fe4ae2bd39d287a75fd7b083", "555": "a1b39e939f509b90a6b4f58c0c846cdfd22ae8fc", "592": "7f92cbca1bab219847ea7c9b6ccfced6b3de3565", "611": "357ead11b55196b3524196164733d2b072e09449", "619": "cf1a7326d8d648f75ec0daa3044fef4dfa71c7c5", "628": "bdf81500fef77a27f59d0541a8405ff3dfe44029", "648": "d7b6e1b40613f6b4b559a6777d1ac13e63e280bc", "697": "0a87f5c08de754cfcecf8261ad2c74119069ae65", "743": "884eedc27666ab6b208a5bb7b4cc9bdb492ff58e", "806": "3c98e789888d71133a5cccb1ad6d27a0d766f316", "813": "2f25723f0570eacc63e03fc09de6e9106b61dc4a", "826": "0f3dd263209c35022a0eac43583e11c7c38742fc", "827": "1bb8fa063f725ad00282ff18eaea3754c8b085da", "838": "09aaf7b32b5f5750e1515e7ffaea3779d5e051dc", "859": "7a449cddb6e7a2164ea03912a934d423a3925455", "877": "beab8372e6b276a57f4f6d6384b1e7bd8dc59704", "885": "7baa141aa4a73dea538d6b257dda2f334c85bb46", "901": "a54daaa294d571a2adbca6acf96edd3c3b550d96", "913": "1e1d0d70542c2df9f5dc844c037b5136f77771dd", "921": "0df1889b2e3cef53c00ccc10155186a31341e0ce", "970": "5f61b27cd7898f376418e2ff7b4e0fcc61e712fc", "1015": "d4fb4834d5645785a2999381572456e2e1a0e2bc", "1021": "bccff006ed94d8a7530367daf67487a2ae2d471a", "1024": "7203c11b2dc111c6f5bc67d42ce66d48fdbafa05", "1032": "1e5b739d56edf61af3bb4d572706262432f72633", "1033": "4f5a6db83710619cb98645cca8b375778c3b7872", "1117": "91994f71840b1b2637d5dd0a3ea16d13d3640dfb", "1148": "8524cbdd76c99d5d4c580f0abc31fa52c9bcc922", "1154": "7b5853c9f38428ae84ce65c6703fd9eb03d7979d", "1274": "8c50cab75bb0a2360dfdbfe454ecda195a3464b3", "1292": "5a38563045eb98941710984f3a8a39583f23cf2f", "1405": "e5c626b209d61639d21a158b0d5cde1ae9b9a31c", "1419": "ada51233bccdab968692e952d40f1c62323cd713", "1516": "1a90cc53c775fe993b756fcdc8648ae643051780", "1552": "545807a2413a511c4f6ef64f7c679fc32f28e650", "1556": "f60ef8fb69e5c3e2939a0ee5d1f55d9017649a7f", "1584": "91b1d563714b2f3c262ceb17e4f3bd8646415a33", "1585": "05fc1a7f13e8c3cf21ff671f7a942687f88c5b42", "1617": "6b72e8652824b991eceb96aea67b2ca4ee68de3b", "1634": "b36736c7d0e6a6c06b809e755964db5ca138eeef", "1681": "7d8c24e75d6897287ba6b97a2c059af026402941", "1683": "5fab651aaee4dd50e7cf97fe9c3f85bf618c3001", "1704": "66171bedffdd5968432e3a799c506b011a34debe", "1768": "052b8e3c61e64448605171e78dccb9b6da9f1adb", "1825": "de6e3cb6f3856edaf55d5ce2ecab8dc01bb05b6c", "1840": "19d67f36b3c4636846c2825aa07d5c92b4796c99", "1855": "03fb62206e9404360616552a41b1934dfd2eeef5", "1859": "efeb3c277af32e0b3f09a481b3fb360faaed2b6a", "1881": "a8664a622b8f4c1f6aa9cacf2742145da1907d00", "1942": "724ecccd67548b5bbd4acd75bf46b317055448d6", "1946": "c14082dc9b021a0fa48185505a8a2640f98c1910", "1947": "c696f598cb7c9e0ae77227007a86e1868be274ac", "1949": "ad257eb14e59f533c4588723b7c89e2856f02c3f", "1954": "b8cf5c7f45c9c1f8aeacfb9705a9acba77ee247a", "1956": "1c5a41918fd650471750062295c9369e1679ebd4", "1962": "89fc8bb90dcc29fed6e39ca3ddd5b9824c64b2b2", "1966": "9a4fadce8190395c4a873037babce412ced6f987", "1970": "8240ab2220494344868d6a1031b58e3679c16e9b", "1976": "47967de95128ad6f69a994bfbf82c1f156319f5f", "1977": "b2c820c700eeb48514c76fd329995217c17118f9", "1984": "cd632fc84df936930989dc3e77ca541927cd8d21", "1990": "13b807c06ab950134a861efc8f8fdfa46076a6db", "1992": "65cb44d99e29a1dcedad76fbcda6af0e6ce6c7eb", "1999": "72f3730d81b3d1d86fd98832c825936f4a290e5b", "2000": "ac846a89dc11130c590798c4d8fc1ae690a37783", "2002": "5af60035e3eb9b74949ba1791e6e83c8d2cc6699", "2010": "e0aa4eaa54ab27907594bf4c6038f64bebf37509", "2011": "efd869096fb69d07ba6bd5635b3e64e799283c5c", "2013": "bf1d7885241e145b7e2c95b0ac49d107f4c873d3", "2015": "0bf5d1b74067ef4fa870f3d7665ec6d792b99d06", "2040": "356972d3c4e78efb84602d3f764dc58bf77d5bbf", "2046": "911a55c5543626d7c0da39e6db2e7d39d716cf60", "2054": "ff4309d06fe2fa506d574264dfccfcb9e7306a69", "2057": "7205a86a8ce0ef45066d78f0357f5e08031a157b", "2062": "9e3a47dd0efd27e37a4047142612b17da4cd9ddc", "2070": "03ff0dc0cde52481a3f2ab2243c35d3c26813322", "2076": "67c78fbfa02e3648ba52c0b76b83659a64696d01", "2077": "b367e7e9b4bfcb1c848a1db92352e8de2fd91bd9", "2083": "1cd7d33bf88f7148ae0aad8f525d5f0f4392147c", "2084": "483019a95f2d841b99ddf9cdb793dfa21398151f", "2095": "841bf40d284b0e5064eae47102d1800de2a446fe", "2097": "c81f8ad2b8f0831cfa855e6b3dc954b180c4c941", "2100": "9b2cbb9f1bf58b70f09370cd74de691083f8e6f6", "2111": "f46592d05942605da6336f8dbf9cf671d5a1d2d4", "2119": "8cf8279b45649f03582202e169aeb6a0f2978732", "2121": "cf391165aed6a3b89b61338c5b4c7f4e0ce92b44", "2124": "31853f18bd5b4ec291515860f9f572d01d6e67b6", "2129": "66a5d19c4a2c3b2423b52a68f984ca8d9b52afce", "2134": "dc6764fb5ef2d159f9f2e35bcf9c68493d94b140", "2139": "cb607222a64cc53012c65ebfbdbbb98090c1125b", "2143": "d404cdcbcd181dc021a162ed589373f44ee74bd1", "2153": "dfeb11207051437388e4574ab2b9636893415150", "2154": "083e0f602910ad86524d719cab1ae367eb06a5a5", "2173": "7bf8b4aa087ed4226d99111ceab18095da1d358c", "2184": "d0f7487f162fcc22239ecd84ae705773cdf9ef51", "2195": "51b8e2a327c9facebe27289946a6d80c38a67982", "2197": "eac6d09dfe67665b1b430569c91f91961987e027", "2201": "a21da2371ca7875f39ff6d14462ed16d0520b9c9", "2216": "b9aabf5942884764f6c641cd9429a5d6f47cd6a1", "2217": "471d89912c91fdae1d2c507f10d5fedd234f9edf", "2218": "8637fa2da367cf39cea88c27ab79a17f9ac308c3", "2229": "86e9f769e3fb555f0037674672c75cd3120132f7", "2231": "bbf594f8c200231bcf50a81c64e396bb42d19cda", "2238": "2881422c59fc8dd0f070daec9d3f495627f68b06", "2246": "6fc2aaa50fff579b178413a7bfd4b5c9c811e214", "2255": "0fc441cf64d46994cdb5b766e68c3d81b60e0086", "2256": "dd4617e309c5b31cebe79816ea43bf1136b59365", "2268": "683d833a675bd18b38417f104b2d0d5e6b4e3d97", "2271": "89817db81728b323ca296ba18a957c8aacfe11bb", "2291": "28feeb340c52664d5f9aec1363f47a499a0bf40e", "2297": "2858895ea4d1b2e2ec1602067c1bf16d88f9a8c7", "2298": "273b43f6b0f6cad1d490d917e2b1a150089c5683", "2303": "ec65b8e5f2d3bd556ecd1e3d73cb23705cafd7e8", "2313": "498c3311dc08f5c8872ccac0ef3dfb278776699c", "2326": "102b1c0ef09db96014a65d6fea007854744e0906"}, "revision_to_date": {"42": 1678962043000, "214": 1683553743000, "292": 1684752617000, "308": 1685003160000, "436": 1686329453000, "499": 1687514692000, "504": 1687872752000, "506": 1687875060000, "508": 1687875381000, "555": 1689023430000, "592": 1690274961000, "611": 1690383063000, "619": 1690877437000, "628": 1691153153000, "648": 1691507415000, "697": 1692029284000, "743": 1693225318000, "806": 1693832948000, "813": 1693959700000, "826": 1694183836000, "827": 1694192794000, "838": 1694691587000, "859": 1695302465000, "877": 1695645568000, "885": 1695748368000, "901": 1695993535000, "913": 1696342948000, "921": 1696861881000, "970": 1697796332000, "1015": 1698689107000, "1021": 1698773717000, "1024": 1698788569000, "1032": 1698844752000, "1033": 1698846082000, "1117": 1700501108000, "1148": 1700745145000, "1154": 1700831897000, "1274": 1702383559000, "1292": 1702566689000, "1405": 1705055585000, "1419": 1705330362000, "1516": 1706109394000, "1552": 1706519986000, "1556": 1706624458000, "1584": 1707226614000, "1585": 1707227524000, "1617": 1707409032000, "1634": 1707819609000, "1681": 1708946786000, "1683": 1708947852000, "1704": 1709033907000, "1768": 1710379468000, "1825": 1711382694000, "1840": 1711611193000, "1855": 1712137847000, "1859": 1712215927000, "1881": 1713175869000, "1942": 1714565416000, "1946": 1714829539000, "1947": 1714495986000, "1949": 1715088318000, "1954": 1715251867000, "1956": 1715273959000, "1962": 1715370783000, "1966": 1715672695000, "1970": 1715777261000, "1976": 1715784844000, "1977": 1712846843000, "1984": 1715950456000, "1990": 1716292993000, "1992": 1716453922000, "1999": 1716997758000, "2000": 1717087554000, "2002": 1717170495000, "2010": 1717414433000, "2011": 1717414901000, "2013": 1717505162000, "2015": 1717513301000, "2040": 1717592919000, "2046": 1718036933000, "2054": 1718120600000, "2057": 1718196636000, "2062": 1718291838000, "2070": 1718729150000, "2076": 1718802776000, "2077": 1718792786000, "2083": 1718980969000, "2084": 1719219102000, "2095": 1719324822000, "2097": 1719326066000, "2100": 1719403203000, "2111": 1719565491000, "2119": 1719590331000, "2121": 1719779356000, "2124": 1719857742000, "2129": 1719997980000, "2134": 1720163872000, "2139": 1720442209000, "2143": 1720508932000, "2153": 1720715859000, "2154": 1720770981000, "2173": 1721060703000, "2184": 1721117007000, "2195": 1721321907000, "2197": 1721729324000, "2201": 1721744859000, "2216": 1721812779000, "2217": 1721819059000, "2218": 1721825228000, "2229": 1722001245000, "2231": 1722271303000, "2238": 1722346568000, "2246": 1722432803000, "2255": 1722514049000, "2256": 1722514119000, "2268": 1722592500000, "2271": 1722602989000, "2291": 1722855272000, "2297": 1722935151000, "2298": 1722939960000, "2303": 1722440540000, "2313": 1723236605000, "2326": 1723308188000}, "params": {"machine": ["ArcticDB-Medium-Runner"], "python": ["3.6"], "version": [1, null], "branch": ["master"]}, "graph_param_list": [{"machine": "ArcticDB-Medium-Runner", "python": "3.6", "branch": "master", "version": null}, {"machine": "ArcticDB-Medium-Runner", "python": "3.6", "version": 1, "branch": "master"}], "benchmarks": {"basic_functions.BasicFunctions.peakmem_read": {"code": "class BasicFunctions:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "2aac10521743bd6cc38b0651c5889d474b5468c1166994261a08aa12c090e354"}, "basic_functions.BasicFunctions.peakmem_read_short_wide": {"code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "8dc0558241a4d313bcc9c607e79ceae1848864130859c3d5512889303c5b1ab2"}, "basic_functions.BasicFunctions.peakmem_read_with_columns": {"code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_with_columns", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "514794820c0bbf4a1c82aaaf47090ca4ed7a6834bc59a8d30757623798f409f7"}, "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {"code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "a88b0d9a342fd1be6fd7564bfcac0597bd9046ebe0d4ddd01ef4c10ac95326f6"}, "basic_functions.BasicFunctions.peakmem_write": {"code": "class BasicFunctions:\n    def peakmem_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_write", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "8b2b9b8fa4fff3c9b39e8282371bbb03daf0cfda234931598dc9103ef557e87c"}, "basic_functions.BasicFunctions.peakmem_write_short_wide": {"code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_write_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "22e1d7860c2f63edbe004b56bb87f88495e8b012c7b802a4ff08a12b141c434a"}, "basic_functions.BasicFunctions.peakmem_write_staged": {"code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_write_staged", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "7292f3cb4cf929adc3a00d2fa29bc2c8e1472bd52f1001564cc652d0a9433b04"}, "basic_functions.BasicFunctions.time_read": {"code": "class BasicFunctions:\n    def time_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c9e34f0d193d0b815bb954c03915b389b6e05cdfc18545517b411e335e14e3f8", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_short_wide": {"code": "class BasicFunctions:\n    def time_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8845b4655ab7a2fc0b836da115b599fe8bb7ba8001660b6af543acb5fe765170", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_with_columns": {"code": "class BasicFunctions:\n    def time_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_columns", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "edfa58702d2a4ee0ee84197a7ba81194e5e3aecc0f8520f2a8d4b06d45896021", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_with_date_ranges": {"code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_date_ranges", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "111bbd81442ec1a42e9c28ce665a629c3875d7c4dc1e6771f4cdce9fed080e2b", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write": {"code": "class BasicFunctions:\n    def time_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5be801860fd4852357fa9010f680596a23a606e1f85e672341de2a1b472ce1e1", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write_short_wide": {"code": "class BasicFunctions:\n    def time_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "25b8f595dd9b3b81d5544e539fb0f4c24455015c2ccb2bceb82c28fbf0698680", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write_staged": {"code": "class BasicFunctions:\n    def time_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_staged", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "4e330332407ef3b614f0cafbed8ff8fd944b2ecf9854ab225cef0cda6bb8ce8c", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.peakmem_read_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "8fe50f627f8d9efb117cadb0f26ee0cbe39662ba1b2905cf3984f33fb03b6f6e"}, "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "b260f7db89580432675d89ba8d08e74ff830f74b81cf7803d58ba44068c9a49c"}, "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE)\n            for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "f440048c530dbb90f782985f7c45e774e3cda60f0d134665e4ef6adf02bb18b7"}, "basic_functions.BatchBasicFunctions.peakmem_write_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_write_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "add75de5523f5002ae08e9f3c01ab33698a49635fe16d01031164b203e4fdfe1"}, "basic_functions.BatchBasicFunctions.time_read_batch": {"code": "class BatchBasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "32b2eed380d4e7863cb87b2011f6add3881e424eeae2cd9b11dc75507fbf8640", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.time_read_batch_pure": {"code": "class BatchBasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        self.lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_pure", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2d1bcf173d39e007c6e55c195e669dc59c57c6852e3ec435d30e421f6baa606e", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.time_read_batch_with_columns": {"code": "class BatchBasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_columns", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ee90e0674ceeebbac5b24babbd514a31eecf352b2fd8d232fce2fe886c559da3", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges": {"code": "class BatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE)\n            for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9ce9e0532a98436b27f547980b8b4ec1914e34acab556ec30e8f3727daf7290d", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.time_write_batch": {"code": "class BatchBasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_write_batch", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "6989b158867091ab3cb5c68d4d6418425164561776e8b86e37c24e635dd3b722", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_append_large": {"code": "class ModificationFunctions:\n    def time_append_large(self, rows):\n        self.lib.append(f\"sym\", self.df_append_large)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_large", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "be3be12028b2f1a949589e618252e94a88e5f35b5aa90f5815fd8aaa324c8550", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_append_short_wide": {"code": "class ModificationFunctions:\n    def time_append_short_wide(self, rows):\n        self.lib_short_wide.append(\"short_wide_sym\", self.df_append_short_wide)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "3a2e1e7a4dc518468ba388f560231ac1a1366b212dbd3309e3e877606c5630e8", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_append_single": {"code": "class ModificationFunctions:\n    def time_append_single(self, rows):\n        self.lib.append(f\"sym\", self.df_append_single)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_single", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c7f13a15b9074ab9bdb6f3e47ab97d75708938f005021b7a8fde82fe6902041d", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_delete": {"code": "class ModificationFunctions:\n    def time_delete(self, rows):\n        self.lib.delete(f\"sym\")\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "da4c95139bc0ae404ed6585b9e3398af8ed7e421cefcbeb9ff9ea6a77b85915a", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_delete_short_wide": {"code": "class ModificationFunctions:\n    def time_delete_short_wide(self, rows):\n        self.lib_short_wide.delete(\"short_wide_sym\")\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "12254786f4a42e8bd488f48075cb70eddf4d87c8581271e2e2b526b7940123b9", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_update_half": {"code": "class ModificationFunctions:\n    def time_update_half(self, rows):\n        self.lib.update(f\"sym\", self.df_update_half)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_half", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f56b8677f5b90b49568e6865c0656b734b9b2a8054baa71b78eaed8f53cb3176", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_update_short_wide": {"code": "class ModificationFunctions:\n    def time_update_short_wide(self, rows):\n        self.lib_short_wide.update(\"short_wide_sym\", self.df_update_short_wide)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5db16777228d8de1ab4af9943d1ed0541c0b02c4dbcd888cfa3e26f37eb0215b", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_update_single": {"code": "class ModificationFunctions:\n    def time_update_single(self, rows):\n        self.lib.update(f\"sym\", self.df_update_single)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_single", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "cf62fa8a658e2f2ab16d286992423dd8d69334415ab61600906c6e9dc0185597", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_update_upsert": {"code": "class ModificationFunctions:\n    def time_update_upsert(self, rows):\n        self.lib.update(f\"sym\", self.df_update_upsert, upsert=True)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_upsert", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "80de9b1982a498c300177d02874a8626152eccb57cd0ba4228a5bb168e7608c8", "warmup_time": -1}, "list_functions.ListFunctions.peakmem_list_symbols": {"code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "name": "list_functions.ListFunctions.peakmem_list_symbols", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "efa8557e59868203fde3f8d2921698b505ae7a1ce7ff442b3e4c9bebc9ce2771"}, "list_functions.ListFunctions.peakmem_list_versions": {"code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "name": "list_functions.ListFunctions.peakmem_list_versions", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "30457537b6ea77365ec0021b6f78a618dd0e990631d64cf0ae6b85baddca7081"}, "list_functions.ListFunctions.time_has_symbol": {"code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_has_symbol", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "time", "unit": "seconds", "version": "00a6aba7cd18f9fbbfa18c85961d58a03a291bfe32bf033e8d7b88c7b960da90", "warmup_time": -1}, "list_functions.ListFunctions.time_list_symbols": {"code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_symbols", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "time", "unit": "seconds", "version": "7457ceb57b7adfda687387a4599ff60b20ecb6ef556b80329ad2e8ec433fbb17", "warmup_time": -1}, "list_functions.ListFunctions.time_list_versions": {"code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_versions", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "time", "unit": "seconds", "version": "cc2c68ce66d0087882fffcb8be554f525c3f314c8693a37897d37cc18373f1ff", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "1f8093c32e1c5195eb0efb1004c228524cb54aa35d8c79359b17fc91597391a6"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "fa0a87f2f2956453b825adcdb9e95be6a7e8887b2a66839923aa8a433e296e4e"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "ce91e45ba6ec5f5dcd9499b423014b431774a7d81f07daa90d6c29cb8bc84d02"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "4cad6e9389f20fc4a168893003dff16e0577770525b847e71e3b97f0f9f5ecdd"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "91dfe91e2fd6e9d562d89d8aee902dbb5c2380f3cd0a11eb85229cb375a7ea0b"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "94edfd985cb9746d21b85be1c91e97423797af2faa7a3343ad1c3aa7f9fa4536"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "e047abda334f31dda20959739f2a3816f4dc96c130db00ebb75f5adcb9c14999"}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "1fd26d5df8e3bd47278b0f1acca9528cc0dadba82788af6e3cfd1812058abef9", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "a0f79b58b7744e63b2b7df3562f57094fa4ff3a111c172fbe0b03aec197afec8", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_projection": {"code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c7f842a915ebd3e278a9a5cea838835a804b463451ebec69829afe871adccfcc", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8972136efca70caee7530d031766c4653737a79d09b7c7badaaee274c1caa7da", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "17ef74af58c623de0ce47d10ad9d52ffc8a1b3c3bb2f57d1391dde34f4af4f29", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "509ffd471564124f5ea73eab19903e52e70eba728ea59b97ad6bd5b8544c2e60", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9a923014466d420b857d297f2a8a41983d03d0c3242559a8488a2a9a642440e1", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:62", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9d97dcd98574b9edb2038a9d43166c03fb90874813e5fac9c3a44b51194f3dd9", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:62", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b1364bf72e616201e384c0b7a9f18b03b078e22452929466a06b35fc64a91bd6", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:62", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8f27fb785c7b8b40220191dae6dbb120a49f55e011ae0f7cea6516a47e38c18a", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:62", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ed1d1ccb6458095a627788bfa2b53afa310ca8c8118a6405c91204724c865d6c", "warmup_time": -1}, "resample.Resample.peakmem_resample": {"code": "class Resample:\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)", "name": "resample.Resample.peakmem_resample", "param_names": ["num_rows", "downsampling_factor", "col_type", "aggregation"], "params": [["1000000", "10000000"], ["10", "100", "100000"], ["'bool'", "'int'", "'float'", "'datetime'", "'str'"], ["'sum'", "'mean'", "'min'", "'max'", "'first'", "'last'", "'count'"]], "setup_cache_key": "resample:38", "type": "peakmemory", "unit": "bytes", "version": "e64300ebb5bd625e1a0f3774aadd035e5738b41295ec2a8ce082d2e9add9b580"}, "resample.Resample.time_resample": {"code": "class Resample:\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)", "min_run_count": 2, "name": "resample.Resample.time_resample", "number": 5, "param_names": ["num_rows", "downsampling_factor", "col_type", "aggregation"], "params": [["1000000", "10000000"], ["10", "100", "100000"], ["'bool'", "'int'", "'float'", "'datetime'", "'str'"], ["'sum'", "'mean'", "'min'", "'max'", "'first'", "'last'", "'count'"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "resample:38", "type": "time", "unit": "seconds", "version": "2d10a27f3668632f382e90783829b4bb08cabb656c02754c00d5953ee42f3794", "warmup_time": -1}, "version_chain.IterateVersionChain.time_list_undeleted_versions": {"code": "class IterateVersionChain:\n    def time_list_undeleted_versions(self, num_versions, caching, deleted):\n        self.lib.list_versions(symbol=self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_list_undeleted_versions", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "6bdd43d7f191d2bbbd30ef740909969e25cbe1cec77f1755c5c3ba58a77f2b88", "warmup_time": -1}, "version_chain.IterateVersionChain.time_load_all_versions": {"code": "class IterateVersionChain:\n    def time_load_all_versions(self, num_versions, caching, deleted):\n        self.load_all(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_load_all_versions", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c40fe3123db9e5d6fdf5f35caecaf42d266328deb78c237e293096ae3a4bcf98", "warmup_time": -1}, "version_chain.IterateVersionChain.time_read_alternating": {"code": "class IterateVersionChain:\n    def time_read_alternating(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_alternating", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ec1a61c37c4cc7317cfafe554f3eeb7fe2a426068ec412c1d7c6b78f510f6c45", "warmup_time": -1}, "version_chain.IterateVersionChain.time_read_from_epoch": {"code": "class IterateVersionChain:\n    def time_read_from_epoch(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_from_epoch", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5c6aace0b39c7a75f064a61c182cbbb42a35f0e0ee46546579bc641e68dc954a", "warmup_time": -1}, "version_chain.IterateVersionChain.time_read_v0": {"code": "class IterateVersionChain:\n    def time_read_v0(self, num_versions, caching, deleted):\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_v0", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "4bf693e490128c1cff7500c93799432e7bf150925d3714757219604aa7fa5e9c", "warmup_time": -1}}, "machines": {"ArcticDB-Medium-Runner": {"machine": "ArcticDB-Medium-Runner", "python": "3.6", "version": 1}}, "tags": {"1.0.1": 42, "v1.0.1": 42, "v1.1.0": 214, "v1.2.0": 292, "v1.2.1": 308, "v1.3.0": 436, "v1.4.0": 499, "v1.4.1": 504, "v1.4.1-pre.seatontst": 506, "v1.4.1-pre.seatontst.2": 508, "v1.5.0": 555, "v1.6.0": 592, "v1.6.1": 648, "v1.6.1-rc0": 611, "v1.6.1-rc1": 619, "v1.6.1rc2": 628, "v1.6.2": 913, "v1.6.2rc0": 901, "v2.0.0": 743, "v2.0.0rc0": 697, "v3.0.0": 826, "v3.0.0rc0": 806, "v3.0.0rc1": 813, "v3.0.0rc2": 827, "v4.0.0": 885, "v4.0.0rc0": 838, "v4.0.0rc1": 859, "v4.0.0rc2": 877, "v4.0.1": 970, "v4.0.2": 1032, "v4.0.2rc0": 1021, "v4.0.3": 1148, "v4.0.4": 1768, "v4.0.4-docs": 1768, "v4.0.5": 2015, "v4.1.0": 1033, "v4.1.0-docs": 1117, "v4.1.0rc0": 921, "v4.1.0rc1": 1015, "v4.1.0rc2": 1024, "v4.2.0": 1274, "v4.2.0-docs": 1274, "v4.2.0rc0": 1154, "v4.2.1": 1292, "v4.2.1-docs": 1419, "v4.3.0": 1584, "v4.3.0-docs": 1556, "v4.3.0rc0": 1516, "v4.3.0rc1": 1552, "v4.3.0rc2": 1585, "v4.3.1": 1617, "v4.3.1-docs": 1634, "v4.3.2rc0": 1681, "v4.3.2rc1": 1683, "v4.3.2rc2": 1704, "v4.4.0": 1859, "v4.4.0-docs": 1859, "v4.4.0rc0": 1825, "v4.4.0rc1": 1840, "v4.4.0rc2": 1855, "v4.4.1": 1881, "v4.4.1-docs": 1881, "v4.4.2": 1949, "v4.4.2-docs": 1949, "v4.4.2rc0": 1946, "v4.4.3": 2076, "v4.4.3-docs": 2076, "v4.4.3rc0": 1976, "v4.4.3rc1": 2011, "v4.4.4rc0": 2095, "v4.4.4rc1": 2121, "v4.4.4rc2": 2143, "v4.4.4rc3": 2197, "v4.4.4rc4": 2217, "v4.4.4rc5": 2268, "v4.4.4rc6": 2298, "v4.5.0rc0": 2084, "v4.5.0rc1": 2111, "v4.5.0rc2": 2218, "v4.5.0rc3": 2256, "v4.5.0rc4": 2297, "vtop_level_imports-docs": 1405, "bisection": 1954}, "pages": [["", "Grid view", "Display as a agrid"], ["summarylist", "List view", "Display as a list"], ["regressions", "Show regressions", "Display information about recent regressions"]]}